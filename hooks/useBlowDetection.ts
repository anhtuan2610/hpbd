"use client";

import { useEffect, useRef, useState } from "react";

export function useBlowDetection(
  onBlowDetected: () => void,
  threshold: number = 0.5, // Ng∆∞·ª°ng √¢m l∆∞·ª£ng ƒë·ªÉ ph√°t hi·ªán th·ªïi
  sensitivity: number = 0.7 // ƒê·ªô nh·∫°y (0-1)
) {
  const [isListening, setIsListening] = useState(false);
  const [hasPermission, setHasPermission] = useState(false);
  const [error, setError] = useState<string | null>(null);
  const [isLoading, setIsLoading] = useState(false);
  const [permissionStatus, setPermissionStatus] = useState<
    "prompt" | "granted" | "denied" | "unknown"
  >("unknown");
  const audioContextRef = useRef<AudioContext | null>(null);
  const analyserRef = useRef<AnalyserNode | null>(null);
  const microphoneRef = useRef<MediaStreamAudioSourceNode | null>(null);
  const streamRef = useRef<MediaStream | null>(null);
  const animationFrameRef = useRef<number | null>(null);
  const dataArrayRef = useRef<Uint8Array | null>(null);
  const lastBlowTimeRef = useRef<number>(0);
  const BLOW_COOLDOWN = 500; // Cooldown 500ms ƒë·ªÉ tr√°nh spam

  const startListening = async () => {
    try {
      setError(null);
      setIsLoading(true);
      console.log("üé§ ƒêang y√™u c·∫ßu quy·ªÅn truy c·∫≠p microphone...");

      // Ki·ªÉm tra secure context (HTTPS ho·∫∑c localhost)
      const isSecureContext =
        location.protocol === "https:" ||
        location.hostname === "localhost" ||
        location.hostname === "127.0.0.1" ||
        location.hostname === "[::1]";

      if (!isSecureContext) {
        // Khi truy c·∫≠p qua IP network (192.168.x.x), kh√¥ng ph·∫£i secure context
        // Tr√¨nh duy·ªát s·∫Ω kh√¥ng cho ph√©p truy c·∫≠p microphone
        throw new Error(
          `Microphone y√™u c·∫ßu HTTPS ho·∫∑c localhost ƒë·ªÉ ho·∫°t ƒë·ªông.\n\nB·∫°n ƒëang truy c·∫≠p qua: ${location.protocol}//${location.hostname}\n\nGi·∫£i ph√°p:\n1. Truy c·∫≠p qua localhost: http://localhost:3000\n2. Ho·∫∑c d√πng HTTPS (khi deploy)\n3. Ho·∫∑c c·∫•u h√¨nh HTTPS cho local development`
        );
      }

      // Ki·ªÉm tra xem tr√¨nh duy·ªát c√≥ h·ªó tr·ª£ getUserMedia kh√¥ng
      // H·ªó tr·ª£ c·∫£ API m·ªõi (mediaDevices.getUserMedia) v√† API c≈© (navigator.getUserMedia)
      const nav = navigator as any;
      const getUserMedia =
        navigator.mediaDevices?.getUserMedia ||
        nav.getUserMedia ||
        nav.webkitGetUserMedia ||
        nav.mozGetUserMedia;

      if (!getUserMedia) {
        throw new Error(
          "Tr√¨nh duy·ªát kh√¥ng h·ªó tr·ª£ microphone. Vui l√≤ng d√πng Chrome, Firefox ho·∫∑c Safari."
        );
      }

      // Y√™u c·∫ßu quy·ªÅn truy c·∫≠p microphone
      let stream: MediaStream;

      if (navigator.mediaDevices?.getUserMedia) {
        // S·ª≠ d·ª•ng API m·ªõi
        stream = await navigator.mediaDevices.getUserMedia({
          audio: {
            echoCancellation: false,
            noiseSuppression: false,
            autoGainControl: false,
          },
        });
      } else {
        // Fallback cho API c≈©
        stream = await new Promise<MediaStream>((resolve, reject) => {
          const oldGetUserMedia =
            (navigator as any).getUserMedia ||
            (navigator as any).webkitGetUserMedia ||
            (navigator as any).mozGetUserMedia;

          oldGetUserMedia.call(navigator, { audio: true }, resolve, reject);
        });
      }

      streamRef.current = stream;
      setHasPermission(true);
      setIsListening(true);
      setIsLoading(false);
      setError(null);
      console.log("‚úÖ ƒê√£ c√≥ quy·ªÅn truy c·∫≠p microphone");

      // T·∫°o AudioContext
      const audioContext = new (window.AudioContext ||
        (window as any).webkitAudioContext)();
      audioContextRef.current = audioContext;

      // T·∫°o AnalyserNode ƒë·ªÉ ph√¢n t√≠ch audio
      const analyser = audioContext.createAnalyser();
      analyser.fftSize = 256; // K√≠ch th∆∞·ªõc FFT (Fast Fourier Transform)
      analyser.smoothingTimeConstant = 0.3;
      analyserRef.current = analyser;

      // K·∫øt n·ªëi microphone v·ªõi analyser
      const microphone = audioContext.createMediaStreamSource(stream);
      microphone.connect(analyser);
      microphoneRef.current = microphone;

      // T·∫°o m·∫£ng ƒë·ªÉ l∆∞u d·ªØ li·ªáu audio
      const bufferLength = analyser.frequencyBinCount;
      const dataArray = new Uint8Array(bufferLength);
      dataArrayRef.current = dataArray as any;

      console.log("üéß B·∫Øt ƒë·∫ßu ph√¢n t√≠ch audio...");

      // H√†m ph√¢n t√≠ch audio li√™n t·ª•c
      const analyze = () => {
        if (!analyserRef.current || !dataArrayRef.current) return;

        analyserRef.current.getByteFrequencyData(dataArrayRef.current as any);

        // T√≠nh to√°n t·ªïng nƒÉng l∆∞·ª£ng ·ªü t·∫ßn s·ªë th·∫•p (ti·∫øng th·ªïi th∆∞·ªùng ·ªü 20-200Hz)
        // Ch·ªâ l·∫•y 10 bin ƒë·∫ßu ti√™n (t·∫ßn s·ªë th·∫•p)
        let sum = 0;
        const lowFreqBins = Math.min(10, dataArrayRef.current.length);
        for (let i = 0; i < lowFreqBins; i++) {
          sum += dataArrayRef.current[i];
        }
        const average = sum / lowFreqBins / 255; // Normalize v·ªÅ 0-1

        // Log gi√° tr·ªã ƒë·ªÉ debug (ch·ªâ log khi c√≥ √¢m thanh)
        if (average > 0.1) {
          console.log(`üìä Audio level: ${(average * 100).toFixed(2)}%`);
        }

        // Ph√°t hi·ªán ti·∫øng th·ªïi: amplitude cao ·ªü t·∫ßn s·ªë th·∫•p
        const now = Date.now();
        if (
          average > threshold * sensitivity &&
          now - lastBlowTimeRef.current > BLOW_COOLDOWN
        ) {
          lastBlowTimeRef.current = now;
          console.log("üí® PH√ÅT HI·ªÜN TI·∫æNG TH·ªîI! (Blow detected!)");
          onBlowDetected();
        }

        animationFrameRef.current = requestAnimationFrame(analyze);
      };

      analyze();
    } catch (err: any) {
      console.error("‚ùå L·ªói khi truy c·∫≠p microphone:", err);
      setHasPermission(false);
      setIsListening(false);
      setIsLoading(false);

      // X·ª≠ l√Ω c√°c lo·∫°i l·ªói kh√°c nhau
      if (
        err.name === "NotAllowedError" ||
        err.name === "PermissionDeniedError"
      ) {
        setPermissionStatus("denied");
        setError(
          "B·∫°n ƒë√£ t·ª´ ch·ªëi quy·ªÅn truy c·∫≠p microphone. Vui l√≤ng click v√†o icon üîí ho·∫∑c üé§ ·ªü thanh ƒë·ªãa ch·ªâ tr√¨nh duy·ªát v√† cho ph√©p microphone, sau ƒë√≥ l√†m m·ªõi trang."
        );
      } else if (
        err.name === "NotFoundError" ||
        err.name === "DevicesNotFoundError"
      ) {
        setError(
          "Kh√¥ng t√¨m th·∫•y microphone. Vui l√≤ng ki·ªÉm tra thi·∫øt b·ªã c·ªßa b·∫°n."
        );
      } else if (
        err.name === "NotReadableError" ||
        err.name === "TrackStartError"
      ) {
        setError(
          "Microphone ƒëang ƒë∆∞·ª£c s·ª≠ d·ª•ng b·ªüi ·ª©ng d·ª•ng kh√°c. Vui l√≤ng ƒë√≥ng ·ª©ng d·ª•ng ƒë√≥."
        );
      } else {
        setError(err.message || "L·ªói kh√¥ng x√°c ƒë·ªãnh khi truy c·∫≠p microphone.");
      }
    }
  };

  const stopListening = () => {
    if (animationFrameRef.current) {
      cancelAnimationFrame(animationFrameRef.current);
      animationFrameRef.current = null;
    }

    if (streamRef.current) {
      streamRef.current.getTracks().forEach((track) => track.stop());
      streamRef.current = null;
    }

    if (audioContextRef.current) {
      audioContextRef.current.close();
      audioContextRef.current = null;
    }

    setIsListening(false);
    console.log("üõë ƒê√£ d·ª´ng nghe microphone");
  };

  useEffect(() => {
    return () => {
      stopListening();
    };
  }, []);

  return {
    startListening,
    stopListening,
    isListening,
    hasPermission,
    error,
    isLoading,
    permissionStatus,
  };
}
